{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - Competitive Assignment\n",
    "<p style=\"font-family: Arial; font-size:1em;color:blue;\">Student Name: Or Ben-Aba</p>\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preceding Step - import modules (packages)\n",
    "This step is necessary in order to use external modules (packages). <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# tokenizer\n",
    "import hebrew_tokenizer as ht\n",
    "# vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# evaluation score\n",
    "from sklearn.metrics import f1_score\n",
    "# standard\n",
    "import numpy as np\n",
    "# ignore printted warnings\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:2em;color:orange;font-weight:bold;\">Reading input files</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = '.' + os.sep + 'input' + os.sep + 'annotated_corpus_for_train.xlsx'\n",
    "test_filename  = '.' + os.sep + 'input' + os.sep + 'corpus_for_test.xlsx'\n",
    "df_train = pd.read_excel(train_filename, 'corpus', index_col=None, na_values=['NA'])\n",
    "df_test  = pd.read_excel(test_filename,  'corpus', index_col=None, na_values=['NA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>בוקר אחד קמתי סהרורי יצאתי מהמיטה קצת מטושטש ,...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>לחבר שלי היה יום הולדת וחיפשנו מה אפשר לעשות ל...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>השנה האחרונה הייתה שנת קורונה, שנה לא פשוטה בק...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>לפני כחצי שנה עברתי לגור בצפון עם בת זוגתי, עב...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>יום חמישי רגיל, תמיד מתחיל לעבור טיפה מאוחר יו...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>לפני כ3 שנים התגוררתי למשך שנה בגרמניה. מטרת ה...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>ביום הבחירות נסענו לבקר את אימי ז\"ל בבית הקברו...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>בשנה אחרונה חוויתי לראשונה את תהליך חיפוש העבו...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>אני סטודנט במכללה, בסמסטר א בשנת תשפ\"א נגשתי ל...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>‏הייתי מדריכה בכפר נוער ומתאם הכפר היינו צריכי...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>364 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 story gender\n",
       "0    בוקר אחד קמתי סהרורי יצאתי מהמיטה קצת מטושטש ,...      m\n",
       "1    לחבר שלי היה יום הולדת וחיפשנו מה אפשר לעשות ל...      m\n",
       "2    השנה האחרונה הייתה שנת קורונה, שנה לא פשוטה בק...      m\n",
       "3    לפני כחצי שנה עברתי לגור בצפון עם בת זוגתי, עב...      m\n",
       "4    יום חמישי רגיל, תמיד מתחיל לעבור טיפה מאוחר יו...      m\n",
       "..                                                 ...    ...\n",
       "359  לפני כ3 שנים התגוררתי למשך שנה בגרמניה. מטרת ה...      m\n",
       "360  ביום הבחירות נסענו לבקר את אימי ז\"ל בבית הקברו...      m\n",
       "361  בשנה אחרונה חוויתי לראשונה את תהליך חיפוש העבו...      m\n",
       "362  אני סטודנט במכללה, בסמסטר א בשנת תשפ\"א נגשתי ל...      m\n",
       "363  ‏הייתי מדריכה בכפר נוער ומתאם הכפר היינו צריכי...      f\n",
       "\n",
       "[364 rows x 2 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:2em;color:orange;font-weight:bold;\">Constants</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_irrelevant_words = []\n",
    "with open(\"irrelevant_words.txt\", \"r\", encoding=\"utf8\") as f_irrelevant_words:\n",
    "    global arr_irrelevant_words\n",
    "    arr_irrelevant_words = (f_irrelevant_words.read()).split()\n",
    "\n",
    "\n",
    "\n",
    "## Boost words. Important words that testify that a word is unique for one gender \n",
    "arr_boost_words = []\n",
    "with open(\"boost_words.txt\", \"r\", encoding=\"utf8\") as f_boost_words:\n",
    "    global arr_boost_words\n",
    "    arr_boost_words = (f_boost_words.read()).split()\n",
    "    \n",
    "    \n",
    "multiplyer = 25\n",
    "# enum\n",
    "from enum import Enum\n",
    "class Gender(Enum):\n",
    "    FEMALE = 0\n",
    "    MALE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:2em;color:orange;font-weight:bold;\">Functions</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# << @@Params (long string):\n",
    "# <<                        one text story. <<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# << @@Return Value: (Array of strings)                      <<<<<<<<<<<<\n",
    "# <<    All the textual words in hebrew, without numbers ... <<<<<<<<<<<<\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def tokenization(story):\n",
    "    story_tokens = []\n",
    "    # tokenizer generator\n",
    "    tokens = ht.tokenize(story)\n",
    "    story_tokens = [token for grp, token, _, (_, _) in tokens if grp == \"HEBREW\"]\n",
    "    return story_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# << @@Params (Two DataFrames):\n",
    "# <<             Train & Test DataFrames <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# << @@Return Value: (Two DataFrame)                      <<<<<<<<<<<<<<<\n",
    "# <<    Train & Test Data Frames After Count Vectorization <<<<<<<<<<<<<<\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def vectorization(df_train, df_test):\n",
    "    # max_features is intended yo limit the features in the data frame\n",
    "    vec = CountVectorizer(stop_words=arr_irrelevant_words, max_features=10000)\n",
    "    # fit the vectorizer with the train columns \n",
    "    vectorized_train = vec.fit_transform(df_train)\n",
    "    # then count these features in the test set  \n",
    "    vectorized_test = vec.transform(df_test)\n",
    "    # creating new dataFrames without the shitty stop words, timing words, places words (irrelevant words)\n",
    "    vectorized_train = pd.DataFrame(vectorized_train.toarray(), columns=vec.get_feature_names())\n",
    "    vectorized_test = pd.DataFrame(vectorized_test.toarray(), columns=vec.get_feature_names())\n",
    "    return vectorized_train, vectorized_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:2em;color:blue;font-weight:bold;\">Data Preprocssing</p>\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;font-weight:bold;\">There are two main approaches: Black List Approach, White List Approach</p>\n",
    "\n",
    "<!-- Explanation -->\n",
    "<p style=\"font-family: Arial; font-size:1em;color:black;\">\n",
    "Black List: Remove all the problematic words that do not refer to any gender.\n",
    "Namely, removing all: Stop Words, Activation Words, Noun Verbs, places, names etc.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:green;font-weight:bold;\">Black List Approach</p>\n",
    "<p style=\"font-family: Arial; font-size:1em;color:black;\">\n",
    "Using Regexes in order to recongnize Activation words, Noun verbs and other non-meaningful words that not relate to one gender.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# מחיקת המילים שנמצאות בגוף ראשון מדבר:\n",
    "def is_first_body(word):    \n",
    "    # בניין קל: --> א**ו* (ארשום)\n",
    "    # בניין הפעיל: --> א**י*, או*י* (אלביש, אוסיף)\n",
    "    # בניין התפעל: --> את*** (אתלבש)\n",
    "    for regex in [\"^א..ו.$\", \"^א..י.$\", \"^או*י*$\", \"^את...$\"]:\n",
    "        if bool(re.match(regex, word)):\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "# מחיקת שמות פעולה\n",
    "def is_activation_word(word):\n",
    "    # בניין התפעל: (התנצלות)\n",
    "    regex = \"^הת...ות$\"\n",
    "    return bool(re.match(regex, word))\n",
    "\n",
    "# If the word ends with vowel\n",
    "# or\n",
    "# the word ends with תיות or יים\n",
    "# or\n",
    "# the word is first bosy word\n",
    "def is_shitty_word(word):\n",
    "    return (word[-1] == 'ו' or word[-1] == 'י') or is_activation_word(word) or ((len(word) > 3 and word[-4:] == \"תיות\") or (len(word) > 2 and word[-3:] == \"יים\")) or is_activation_word(word)\n",
    "\n",
    "\n",
    "def is_word_need_to_be_removed(word):\n",
    "    return word not in arr_boost_words and is_shitty_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_list_approach(df_train, df_test):\n",
    "    # Using tokenization\n",
    "    tokenized_df_train = [\" \".join(tokenization(story)) for story in df_train.loc[:,\"story\"]]\n",
    "    \n",
    "    tokenized_df_test = [\" \".join(tokenization(story)) for story in df_test.loc[:,\"story\"]]\n",
    "    # Using vectorization\n",
    "    vectorized_df_train, vectorized_df_test = vectorization(tokenized_df_train, tokenized_df_test)\n",
    "\n",
    "    # filtering the shitty words\n",
    "    stayed_train_columns = [col for col in vectorized_df_train.columns if not is_word_need_to_be_removed(col)]\n",
    "    stayed_test_columns = [col for col in vectorized_df_train.columns if not is_word_need_to_be_removed(col)]\n",
    "    \n",
    "    df_train_without_shitty = pd.DataFrame([])\n",
    "    df_test_without_shitty = pd.DataFrame([])\n",
    "    \n",
    "    for col in stayed_train_columns:\n",
    "        df_train_without_shitty[col] = vectorized_df_train[col]\n",
    "\n",
    "    for col in stayed_test_columns:\n",
    "        df_test_without_shitty[col] = vectorized_df_test[col]\n",
    "            \n",
    "    return df_train_without_shitty, df_test_without_shitty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_without_shitty, df_test_without_shitty = black_list_approach(df_train, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.2em;color:green;font-weight:bold;\">White List Approach</p>\n",
    "<p style=\"font-family: Arial; font-size:1em;color:black;\">\n",
    "White List: Relating only to the important words. All the other words are not affect.\n",
    "In this approach, we relate only to the words that connected to one specific gender.\n",
    "Namely, by using Logistic XOR, between the male words group and the female words group in the train,\n",
    "we can infer what words are more \"female\"/\"male\" words. In this approach, we concentrate only on the important words.\n",
    "</p>\n",
    "<p style=\"font-family: Arial; font-size:1em;color:black;\">\n",
    "1) The group of the \n",
    "                    TRAIN_SET_MALE ^ TRAIN_SET_FEMALE\n",
    "is located in the boost_words.txt. These words are very unique and have the chance.\n",
    "Using this technique combining words from hebrew dictionaries.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1em;color:black;\">\n",
    "    2) After looking into the xored words, I started to examine the words in the train test, and I was surprised to \n",
    "    see that my idea is actually working ::)\n",
    "</p>\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1em;color:black;\">\n",
    "    3) The Third step was to add all possible hebrew prefixe, such as: ו כ ל ב ש ה to all the words in the xored result,\n",
    "</p>\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1em;color:black;font-weight:bold;\">\n",
    "    The code for this procedure is commented at the end of this file\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using \"grid search\" in order to find the optimal number to multiply the column by,\n",
    "# and the conclusion is that 25 is the optimal that gives the best results with cross validation\n",
    "for col in df_train_without_shitty.columns:\n",
    "    if col in arr_boost_words:\n",
    "        df_test_without_shitty[col] *= multiplyer\n",
    "        df_train_without_shitty[col] *= multiplyer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:2em;color:blue;font-weight:bold;\">Model Selection</p>\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;font-weight:bold;\">Perceptron, Naive Bayes, SGD</p>\n",
    "\n",
    "<!-- Explanation -->\n",
    "<p style=\"font-family: Arial; font-size:1em;color:black;\">\n",
    "    After hyperparameter tuning, scaling the data, cross validation and grid search for the best results.\n",
    "    I infer that the combination of the three selected models is the best in the gap !!!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the characters from the train data frame\n",
    "Y_train_characterized = df_train.loc[:,'gender']\n",
    "\n",
    "# casting each character to its appropariate gender\n",
    "# f(emale) => 0\n",
    "# m(ale)   => 1\n",
    "Y_train = pd.Series([Gender.FEMALE.value if ch == \"f\"  else Gender.MALE.value for ch in Y_train_characterized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# After hyper-parameter tuning, grid search\n",
    "SGD = SGDClassifier(alpha=0.001, max_iter=50, random_state=1).fit(df_train_without_shitty, Y_train)\n",
    "NB = MultinomialNB(alpha=0.5).fit(df_train_without_shitty, Y_train)\n",
    "percep = Perceptron(max_iter=500).fit(df_train_without_shitty, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "Y_predicted_sgd = SGD.predict(df_test_without_shitty)\n",
    "Y_predicted_NB = NB.predict(df_test_without_shitty)\n",
    "Y_predicted_per = percep.predict(df_test_without_shitty)\n",
    "\n",
    "# Combination of the three models\n",
    "def create_Y_predicted(Y_predicted_sgd, Y_predicted_NB, Y_predicted_per):\n",
    "    Y_predicted = []\n",
    "    for sgd, nb, per in zip(Y_predicted_sgd, Y_predicted_NB, Y_predicted_per):\n",
    "        if per == sgd:\n",
    "            Y_predicted.append(per)\n",
    "        else:\n",
    "            Y_predicted.append(nb)\n",
    "            \n",
    "    return Y_predicted\n",
    "\n",
    "\n",
    "Y_predicted = create_Y_predicted(Y_predicted_sgd, Y_predicted_NB, Y_predicted_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, "
     ]
    }
   ],
   "source": [
    "# printing the predictions\n",
    "for prediction in Y_predicted:\n",
    "    print(prediction, end = \", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:2em;color:blue;font-weight:bold;\">Save Results To CSV</p>\n",
    "After you're done save your output to the 'classification_results.csv' csv file.<br/>\n",
    "We assume that the dataframe with your results contain the following columns:\n",
    "* column 1 (left column): 'test_example_id'  - the same id associated to each of the test stories to be predicted.\n",
    "* column 2 (right column): 'predicted_category' - the predicted gender value for each of the associated story. \n",
    "\n",
    "Assuming your predicted values are in the `df_predicted` dataframe, you should save you're results as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casting the Y_predicted results into the appropriate character gender\n",
    "Y_predicted_characterized = ['m' if n_prediction == Gender.MALE.value else 'f' for n_prediction in Y_predicted]\n",
    "\n",
    "# saving the results as the requested format in the instructions\n",
    "df_result = pd.DataFrame()\n",
    "df_result['test_example_id'] = df_test['test_example_id']\n",
    "df_result['predicted_category'] = Y_predicted_characterized\n",
    "\n",
    "# saving into the CSV\n",
    "df_result.to_csv('classification_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draft code for finding the important words that unique for one gender\n",
    "# # for each story word, we are collecting the gender of the word and statistics\n",
    "# def stories_words(df_train):\n",
    "#     # get the train set\n",
    "#     train_arr_words_and_gender = []\n",
    "#     for index, row in df_train.iterrows():\n",
    "#         story_words_list = tokenization(row.story)\n",
    "#         for word in story_words_list:\n",
    "#             train_arr_words_and_gender.append({word: {row.gender: 1}})\n",
    "        \n",
    "#     return train_arr_words_and_gender\n",
    "\n",
    "# train_arr_words_and_gender = stories_words(df_train, df_test)\n",
    "\n",
    "\n",
    "\n",
    "# def union_duplicated_words(arr_words_and_gender):\n",
    "#     dict_male = {}\n",
    "#     dict_female = {}\n",
    "    \n",
    "#     for t_word in arr_words_and_gender:\n",
    "#         for key in t_word:\n",
    "#             value = t_word[key]\n",
    "#             # if the word is a male word\n",
    "#             if 'm' in value:\n",
    "#                 if not key in dict_male:\n",
    "#                     dict_male[key] = 1\n",
    "#                 else:\n",
    "#                     dict_male[key] += 1\n",
    "#             else:\n",
    "#                 if not key in dict_female:\n",
    "#                     dict_female[key] = 1\n",
    "#                 else:\n",
    "#                     dict_female[key] += 1\n",
    "        \n",
    "#     return dict_male, dict_female\n",
    "\n",
    "\n",
    "# train_dict_male, train_dict_female = union_duplicated_words(train_arr_words_and_gender)\n",
    "\n",
    "\n",
    "# def dict_frequency(dict_male, dict_female, male_sum, female_sum, constant = 10000):\n",
    "#     freq_dict_male = {}\n",
    "#     freq_dict_female = {}\n",
    "    \n",
    "#     for female_word in dict_female:\n",
    "#         value = dict_female[female_word]\n",
    "#         freq_dict_female[female_word] = (value/female_sum) * constant\n",
    "    \n",
    "#     for male_word in dict_male:\n",
    "#         value = dict_male[male_word]\n",
    "#         freq_dict_male[male_word] = (value/male_sum) * constant\n",
    "        \n",
    "#     return freq_dict_male, freq_dict_female\n",
    "    \n",
    "    \n",
    "\n",
    "# train_male_sum = 0\n",
    "# train_female_sum = 0\n",
    "# for word in train_dict_female_new:\n",
    "#     train_female_sum += train_dict_female_new[word]\n",
    "\n",
    "# for word in train_dict_male_new:\n",
    "#     train_male_sum += train_dict_male_new[word]\n",
    "\n",
    "\n",
    "# freq_train_dict_male, freq_train_dict_female = dict_frequency(train_dict_male_new, train_dict_female_new,\n",
    "#                                                                         train_male_sum, train_female_sum)\n",
    "\n",
    "\n",
    "\n",
    "# # find words that are in male words and not in female words and the oposite\n",
    "# def symetric_diff(male_words_dict, female_words_dict, n_times = 10, n_minimum_amount = 1):\n",
    "#     female_special_words = []\n",
    "#     male_special_words = []\n",
    "#     in_intersect = 0\n",
    "#     for female_word in female_words_dict:\n",
    "#         value = female_words_dict[female_word]\n",
    "#         # if the amount of words in the female part is at least n_times of the word in the male part\n",
    "#         if (n_minimum_amount <= value) and (not female_word in male_words_dict or (male_words_dict[female_word] * n_times < value)):\n",
    "#             female_special_words.append({female_word: value})\n",
    "#         else:\n",
    "#             in_intersect += 1\n",
    "            \n",
    "            \n",
    "#     for male_word in male_words_dict:\n",
    "#         value = male_words_dict[male_word]\n",
    "#         # if the amount of words in the female part is at least n_times of the word in the male part\n",
    "#         if (n_minimum_amount <= value) and (not male_word in female_words_dict or (female_words_dict[male_word] * n_times < value)):\n",
    "#             male_special_words.append({male_word: value})\n",
    "#         else:\n",
    "#             in_intersect += 1\n",
    "            \n",
    "            \n",
    "#     print(\"Total words in both genders: \", len(male_words_dict) + len(female_words_dict))\n",
    "#     print(\"Common words: \", in_intersect)\n",
    "#     print(\"Male words: \", len(male_special_words))\n",
    "#     print(\"Female words: \", len(female_special_words))\n",
    "#     return male_special_words, female_special_words\n",
    "    \n",
    "    \n",
    "    \n",
    "# train_male_special_words, train_female_special_words = symetric_diff(freq_train_dict_male, freq_train_dict_female)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# male_words = []\n",
    "# for dic in train_male_special_words:\n",
    "#     for i in dic.keys():\n",
    "#         if len(i)>1:\n",
    "#             male_words.append(i)\n",
    "\n",
    "# female_words = []\n",
    "# for dic in train_female_special_words:\n",
    "#     for i in dic.keys():\n",
    "#         if len(i)>1:\n",
    "#             female_words.append(i)\n",
    "\n",
    "\n",
    "# arrr = []\n",
    "# with open(\"check.txt\", \"r\", encoding=\"utf8\") as f_boost_words:\n",
    "#     global arr_boost_words\n",
    "#     arr_boost_words = (f_boost_words.read()).split()\n",
    "#     for word in arr_boost_words:\n",
    "#         for prefix in [\"לה\", \"בה\", \"שב\", \"וש\", \"כש\"]:\n",
    "#             print(prefix + word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
